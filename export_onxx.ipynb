{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857fe9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ładowanie mapowania z models/policy_network/move_mapping.json...\n",
      "Inicjalizacja modelu (output size: 1792)...\n",
      "Eksportowanie do models/policy_network/CN2_BN2_RLROP.onnx...\n",
      "Wyeskoprtowano do formatu .onxx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_218078/1406171760.py:29: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(model,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from training.policy_network.model import Model \n",
    "\n",
    "MODEL_PATH = \"models/policy_network/CN2_BN2_RLROP.pth\"\n",
    "ONNX_PATH = \"models/policy_network/CN2_BN2_RLROP.onnx\"\n",
    "MAPPING_PATH = \"models/policy_network/move_mapping.json\"\n",
    "\n",
    "def export():\n",
    "    print(f\"Ładowanie mapowania z {MAPPING_PATH}...\")\n",
    "    with open(MAPPING_PATH, \"r\") as f:\n",
    "        mapping = json.load(f)\n",
    "        output_size = len(mapping)\n",
    "\n",
    "    print(f\"Inicjalizacja modelu (output size: {output_size})...\")\n",
    "    model = Model(output_size)\n",
    "    \n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))\n",
    "    model.eval()\n",
    "\n",
    "    dummy_input = torch.randn(1, 13, 8, 8, requires_grad=True)\n",
    "\n",
    "    print(f\"Eksportowanie do {ONNX_PATH}...\")\n",
    "    torch.onnx.export(model, \n",
    "        dummy_input, \n",
    "        ONNX_PATH, \n",
    "        export_params=True,        \n",
    "        opset_version=11,          \n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'], \n",
    "        output_names=['output'],\n",
    "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "    )\n",
    "    \n",
    "    print(\"Wyeksportowano do formatu .onxx\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
