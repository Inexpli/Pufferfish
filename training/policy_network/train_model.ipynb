{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ffe4dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import chess\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12c60842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmdb_dataset import LMDBDataset\n",
    "\n",
    "dataset = LMDBDataset(\"../../data/lmdb/\")\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cdca927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 1., 1., 1., 0., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 1., 1., 1., 0., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(profile=\"full\")\n",
    "\n",
    "x, y = dataset[5]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5bbbace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4505)\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "\n",
    "torch.set_printoptions(profile=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "347499fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilość batchy: 18360\n",
      "ilość pozycji:  2350080\n"
     ]
    }
   ],
   "source": [
    "print(\"Ilość batchy:\", len(loader))\n",
    "print(\"ilość pozycji: \", len(loader)*loader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e9565b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e8743a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from model import ChessPolicyNet\n",
    "\n",
    "model = ChessPolicyNet().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "epochs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c403241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, scaler, path=\"../../models/policy_network/DeltaChess.pt\"):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "\n",
    "    if \"scaler_state\" in checkpoint:\n",
    "        scaler.load_state_dict(checkpoint[\"scaler_state\"])\n",
    "        print(\"Wczytano scaler\")\n",
    "\n",
    "    start_epoch = checkpoint.get(\"epoch\", 0) + 1\n",
    "    print(f\"Wczytano checkpoint z epoki {start_epoch-1}\")\n",
    "\n",
    "    return start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "003d2927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wczytano scaler\n",
      "Wczytano checkpoint z epoki 6\n"
     ]
    }
   ],
   "source": [
    "# start_epoch = 0\n",
    "epochs = 10\n",
    "start_epoch = load_checkpoint(model, optimizer, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23086bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [100/18360], Loss: 1.2670\n",
      "Epoch [7/10], Step [200/18360], Loss: 1.2547\n",
      "Epoch [7/10], Step [300/18360], Loss: 1.2534\n",
      "Epoch [7/10], Step [400/18360], Loss: 1.2707\n",
      "Epoch [7/10], Step [500/18360], Loss: 1.2497\n",
      "Epoch [7/10], Step [600/18360], Loss: 1.2776\n",
      "Epoch [7/10], Step [700/18360], Loss: 1.2504\n",
      "Epoch [7/10], Step [800/18360], Loss: 1.2570\n",
      "Epoch [7/10], Step [900/18360], Loss: 1.2700\n",
      "Epoch [7/10], Step [1000/18360], Loss: 1.2531\n",
      "Epoch [7/10], Step [1100/18360], Loss: 1.2758\n",
      "Epoch [7/10], Step [1200/18360], Loss: 1.2650\n",
      "Epoch [7/10], Step [1300/18360], Loss: 1.2625\n",
      "Epoch [7/10], Step [1400/18360], Loss: 1.2619\n",
      "Epoch [7/10], Step [1500/18360], Loss: 1.2771\n",
      "Epoch [7/10], Step [1600/18360], Loss: 1.2755\n",
      "Epoch [7/10], Step [1700/18360], Loss: 1.2718\n",
      "Epoch [7/10], Step [1800/18360], Loss: 1.2853\n",
      "Epoch [7/10], Step [1900/18360], Loss: 1.2739\n",
      "Epoch [7/10], Step [2000/18360], Loss: 1.2942\n",
      "Epoch [7/10], Step [2100/18360], Loss: 1.2893\n",
      "Epoch [7/10], Step [2200/18360], Loss: 1.2711\n",
      "Epoch [7/10], Step [2300/18360], Loss: 1.2799\n",
      "Epoch [7/10], Step [2400/18360], Loss: 1.2853\n",
      "Epoch [7/10], Step [2500/18360], Loss: 1.2844\n",
      "Epoch [7/10], Step [2600/18360], Loss: 1.2773\n",
      "Epoch [7/10], Step [2700/18360], Loss: 1.2848\n",
      "Epoch [7/10], Step [2800/18360], Loss: 1.2967\n",
      "Epoch [7/10], Step [2900/18360], Loss: 1.2748\n",
      "Epoch [7/10], Step [3000/18360], Loss: 1.3233\n",
      "Epoch [7/10], Step [3100/18360], Loss: 1.2713\n",
      "Epoch [7/10], Step [3200/18360], Loss: 1.2952\n",
      "Epoch [7/10], Step [3300/18360], Loss: 1.3018\n",
      "Epoch [7/10], Step [3400/18360], Loss: 1.2820\n",
      "Epoch [7/10], Step [3500/18360], Loss: 1.2972\n",
      "Epoch [7/10], Step [3600/18360], Loss: 1.3122\n",
      "Epoch [7/10], Step [3700/18360], Loss: 1.2988\n",
      "Epoch [7/10], Step [3800/18360], Loss: 1.3179\n",
      "Epoch [7/10], Step [3900/18360], Loss: 1.2958\n",
      "Epoch [7/10], Step [4000/18360], Loss: 1.3017\n",
      "Epoch [7/10], Step [4100/18360], Loss: 1.3211\n",
      "Epoch [7/10], Step [4200/18360], Loss: 1.3328\n",
      "Epoch [7/10], Step [4300/18360], Loss: 1.3156\n",
      "Epoch [7/10], Step [4400/18360], Loss: 1.3023\n",
      "Epoch [7/10], Step [4500/18360], Loss: 1.3239\n",
      "Epoch [7/10], Step [4600/18360], Loss: 1.3203\n",
      "Epoch [7/10], Step [4700/18360], Loss: 1.3236\n",
      "Epoch [7/10], Step [4800/18360], Loss: 1.2962\n",
      "Epoch [7/10], Step [4900/18360], Loss: 1.3119\n",
      "Epoch [7/10], Step [5000/18360], Loss: 1.3309\n",
      "Epoch [7/10], Step [5100/18360], Loss: 1.3308\n",
      "Epoch [7/10], Step [5200/18360], Loss: 1.3170\n",
      "Epoch [7/10], Step [5300/18360], Loss: 1.3345\n",
      "Epoch [7/10], Step [5400/18360], Loss: 1.3102\n",
      "Epoch [7/10], Step [5500/18360], Loss: 1.3470\n",
      "Epoch [7/10], Step [5600/18360], Loss: 1.3238\n",
      "Epoch [7/10], Step [5700/18360], Loss: 1.3300\n",
      "Epoch [7/10], Step [5800/18360], Loss: 1.3517\n",
      "Epoch [7/10], Step [5900/18360], Loss: 1.3329\n",
      "Epoch [7/10], Step [6000/18360], Loss: 1.3265\n",
      "Epoch [7/10], Step [6100/18360], Loss: 1.3302\n",
      "Epoch [7/10], Step [6200/18360], Loss: 1.3413\n",
      "Epoch [7/10], Step [6300/18360], Loss: 1.3569\n",
      "Epoch [7/10], Step [6400/18360], Loss: 1.3473\n",
      "Epoch [7/10], Step [6500/18360], Loss: 1.3475\n",
      "Epoch [7/10], Step [6600/18360], Loss: 1.3472\n",
      "Epoch [7/10], Step [6700/18360], Loss: 1.3190\n",
      "Epoch [7/10], Step [6800/18360], Loss: 1.3427\n",
      "Epoch [7/10], Step [6900/18360], Loss: 1.3486\n",
      "Epoch [7/10], Step [7000/18360], Loss: 1.3421\n",
      "Epoch [7/10], Step [7100/18360], Loss: 1.3523\n",
      "Epoch [7/10], Step [7200/18360], Loss: 1.3264\n",
      "Epoch [7/10], Step [7300/18360], Loss: 1.3430\n",
      "Epoch [7/10], Step [7400/18360], Loss: 1.3622\n",
      "Epoch [7/10], Step [7500/18360], Loss: 1.3390\n",
      "Epoch [7/10], Step [7600/18360], Loss: 1.3487\n",
      "Epoch [7/10], Step [7700/18360], Loss: 1.3470\n",
      "Epoch [7/10], Step [7800/18360], Loss: 1.3706\n",
      "Epoch [7/10], Step [7900/18360], Loss: 1.3590\n",
      "Epoch [7/10], Step [8000/18360], Loss: 1.3627\n",
      "Epoch [7/10], Step [8100/18360], Loss: 1.3653\n",
      "Epoch [7/10], Step [8200/18360], Loss: 1.3521\n",
      "Epoch [7/10], Step [8300/18360], Loss: 1.3516\n",
      "Epoch [7/10], Step [8400/18360], Loss: 1.3646\n",
      "Epoch [7/10], Step [8500/18360], Loss: 1.3571\n",
      "Epoch [7/10], Step [8600/18360], Loss: 1.3460\n",
      "Epoch [7/10], Step [8700/18360], Loss: 1.3488\n",
      "Epoch [7/10], Step [8800/18360], Loss: 1.3622\n",
      "Epoch [7/10], Step [8900/18360], Loss: 1.3604\n",
      "Epoch [7/10], Step [9000/18360], Loss: 1.3592\n",
      "Epoch [7/10], Step [9100/18360], Loss: 1.3694\n",
      "Epoch [7/10], Step [9200/18360], Loss: 1.3624\n",
      "Epoch [7/10], Step [9300/18360], Loss: 1.3648\n",
      "Epoch [7/10], Step [9400/18360], Loss: 1.3766\n",
      "Epoch [7/10], Step [9500/18360], Loss: 1.3740\n",
      "Epoch [7/10], Step [9600/18360], Loss: 1.3708\n",
      "Epoch [7/10], Step [9700/18360], Loss: 1.3725\n",
      "Epoch [7/10], Step [9800/18360], Loss: 1.3703\n",
      "Epoch [7/10], Step [9900/18360], Loss: 1.3777\n",
      "Epoch [7/10], Step [10000/18360], Loss: 1.3745\n",
      "Epoch [7/10], Step [10100/18360], Loss: 1.4058\n",
      "Epoch [7/10], Step [10200/18360], Loss: 1.3852\n",
      "Epoch [7/10], Step [10300/18360], Loss: 1.3708\n",
      "Epoch [7/10], Step [10400/18360], Loss: 1.3734\n",
      "Epoch [7/10], Step [10500/18360], Loss: 1.3786\n",
      "Epoch [7/10], Step [10600/18360], Loss: 1.3967\n",
      "Epoch [7/10], Step [10700/18360], Loss: 1.3652\n",
      "Epoch [7/10], Step [10800/18360], Loss: 1.3762\n",
      "Epoch [7/10], Step [10900/18360], Loss: 1.3794\n",
      "Epoch [7/10], Step [11000/18360], Loss: 1.3868\n",
      "Epoch [7/10], Step [11100/18360], Loss: 1.3867\n",
      "Epoch [7/10], Step [11200/18360], Loss: 1.3853\n",
      "Epoch [7/10], Step [11300/18360], Loss: 1.3866\n",
      "Epoch [7/10], Step [11400/18360], Loss: 1.3820\n",
      "Epoch [7/10], Step [11500/18360], Loss: 1.3755\n",
      "Epoch [7/10], Step [11600/18360], Loss: 1.3851\n",
      "Epoch [7/10], Step [11700/18360], Loss: 1.3867\n",
      "Epoch [7/10], Step [11800/18360], Loss: 1.3975\n",
      "Epoch [7/10], Step [11900/18360], Loss: 1.3762\n",
      "Epoch [7/10], Step [12000/18360], Loss: 1.3722\n",
      "Epoch [7/10], Step [12100/18360], Loss: 1.3907\n",
      "Epoch [7/10], Step [12200/18360], Loss: 1.4061\n",
      "Epoch [7/10], Step [12300/18360], Loss: 1.3859\n",
      "Epoch [7/10], Step [12400/18360], Loss: 1.3783\n",
      "Epoch [7/10], Step [12500/18360], Loss: 1.4011\n",
      "Epoch [7/10], Step [12600/18360], Loss: 1.4134\n",
      "Epoch [7/10], Step [12700/18360], Loss: 1.3713\n",
      "Epoch [7/10], Step [12800/18360], Loss: 1.3776\n",
      "Epoch [7/10], Step [12900/18360], Loss: 1.3885\n",
      "Epoch [7/10], Step [13000/18360], Loss: 1.3966\n",
      "Epoch [7/10], Step [13100/18360], Loss: 1.3826\n",
      "Epoch [7/10], Step [13200/18360], Loss: 1.3984\n",
      "Epoch [7/10], Step [13300/18360], Loss: 1.4011\n",
      "Epoch [7/10], Step [13400/18360], Loss: 1.3764\n",
      "Epoch [7/10], Step [13500/18360], Loss: 1.4059\n",
      "Epoch [7/10], Step [13600/18360], Loss: 1.4120\n",
      "Epoch [7/10], Step [13700/18360], Loss: 1.4195\n",
      "Epoch [7/10], Step [13800/18360], Loss: 1.4262\n",
      "Epoch [7/10], Step [13900/18360], Loss: 1.4078\n",
      "Epoch [7/10], Step [14000/18360], Loss: 1.4329\n",
      "Epoch [7/10], Step [14100/18360], Loss: 1.3906\n",
      "Epoch [7/10], Step [14200/18360], Loss: 1.3980\n",
      "Epoch [7/10], Step [14300/18360], Loss: 1.4077\n",
      "Epoch [7/10], Step [14400/18360], Loss: 1.4026\n",
      "Epoch [7/10], Step [14500/18360], Loss: 1.4201\n",
      "Epoch [7/10], Step [14600/18360], Loss: 1.3894\n",
      "Epoch [7/10], Step [14700/18360], Loss: 1.4247\n",
      "Epoch [7/10], Step [14800/18360], Loss: 1.4391\n",
      "Epoch [7/10], Step [14900/18360], Loss: 1.4224\n",
      "Epoch [7/10], Step [15000/18360], Loss: 1.4087\n",
      "Epoch [7/10], Step [15100/18360], Loss: 1.4235\n",
      "Epoch [7/10], Step [15200/18360], Loss: 1.4340\n",
      "Epoch [7/10], Step [15300/18360], Loss: 1.4188\n",
      "Epoch [7/10], Step [15400/18360], Loss: 1.4096\n",
      "Epoch [7/10], Step [15500/18360], Loss: 1.4191\n",
      "Epoch [7/10], Step [15600/18360], Loss: 1.3963\n",
      "Epoch [7/10], Step [15700/18360], Loss: 1.4186\n",
      "Epoch [7/10], Step [15800/18360], Loss: 1.4031\n",
      "Epoch [7/10], Step [15900/18360], Loss: 1.4406\n",
      "Epoch [7/10], Step [16000/18360], Loss: 1.4093\n",
      "Epoch [7/10], Step [16100/18360], Loss: 1.4265\n",
      "Epoch [7/10], Step [16200/18360], Loss: 1.4356\n",
      "Epoch [7/10], Step [16300/18360], Loss: 1.4202\n",
      "Epoch [7/10], Step [16400/18360], Loss: 1.4146\n",
      "Epoch [7/10], Step [16500/18360], Loss: 1.4210\n",
      "Epoch [7/10], Step [16600/18360], Loss: 1.4382\n",
      "Epoch [7/10], Step [16700/18360], Loss: 1.4153\n",
      "Epoch [7/10], Step [16800/18360], Loss: 1.4175\n",
      "Epoch [7/10], Step [16900/18360], Loss: 1.4167\n",
      "Epoch [7/10], Step [17000/18360], Loss: 1.4362\n",
      "Epoch [7/10], Step [17100/18360], Loss: 1.4340\n",
      "Epoch [7/10], Step [17200/18360], Loss: 1.4241\n",
      "Epoch [7/10], Step [17300/18360], Loss: 1.4230\n",
      "Epoch [7/10], Step [17400/18360], Loss: 1.4440\n",
      "Epoch [7/10], Step [17500/18360], Loss: 1.4262\n",
      "Epoch [7/10], Step [17600/18360], Loss: 1.4450\n",
      "Epoch [7/10], Step [17700/18360], Loss: 1.4219\n",
      "Epoch [7/10], Step [17800/18360], Loss: 1.4254\n",
      "Epoch [7/10], Step [17900/18360], Loss: 1.4231\n",
      "Epoch [7/10], Step [18000/18360], Loss: 1.4309\n",
      "Epoch [7/10], Step [18100/18360], Loss: 1.4312\n",
      "Epoch [7/10], Step [18200/18360], Loss: 1.4346\n",
      "Epoch [7/10], Step [18300/18360], Loss: 1.4398\n",
      "Epoch [8/10], Step [100/18360], Loss: 1.1705\n",
      "Epoch [8/10], Step [200/18360], Loss: 1.1652\n",
      "Epoch [8/10], Step [300/18360], Loss: 1.1479\n",
      "Epoch [8/10], Step [400/18360], Loss: 1.1606\n",
      "Epoch [8/10], Step [500/18360], Loss: 1.1654\n",
      "Epoch [8/10], Step [600/18360], Loss: 1.1409\n",
      "Epoch [8/10], Step [700/18360], Loss: 1.1740\n",
      "Epoch [8/10], Step [800/18360], Loss: 1.1610\n",
      "Epoch [8/10], Step [900/18360], Loss: 1.1554\n",
      "Epoch [8/10], Step [1000/18360], Loss: 1.1828\n",
      "Epoch [8/10], Step [1100/18360], Loss: 1.1667\n",
      "Epoch [8/10], Step [1200/18360], Loss: 1.1533\n",
      "Epoch [8/10], Step [1300/18360], Loss: 1.1779\n",
      "Epoch [8/10], Step [1400/18360], Loss: 1.1786\n",
      "Epoch [8/10], Step [1500/18360], Loss: 1.1853\n",
      "Epoch [8/10], Step [1600/18360], Loss: 1.1781\n",
      "Epoch [8/10], Step [1700/18360], Loss: 1.1782\n",
      "Epoch [8/10], Step [1800/18360], Loss: 1.1647\n",
      "Epoch [8/10], Step [1900/18360], Loss: 1.1938\n",
      "Epoch [8/10], Step [2000/18360], Loss: 1.1791\n",
      "Epoch [8/10], Step [2100/18360], Loss: 1.1826\n",
      "Epoch [8/10], Step [2200/18360], Loss: 1.1855\n",
      "Epoch [8/10], Step [2300/18360], Loss: 1.1934\n",
      "Epoch [8/10], Step [2400/18360], Loss: 1.1869\n",
      "Epoch [8/10], Step [2500/18360], Loss: 1.2006\n",
      "Epoch [8/10], Step [2600/18360], Loss: 1.2121\n",
      "Epoch [8/10], Step [2700/18360], Loss: 1.1923\n",
      "Epoch [8/10], Step [2800/18360], Loss: 1.1937\n",
      "Epoch [8/10], Step [2900/18360], Loss: 1.2226\n",
      "Epoch [8/10], Step [3000/18360], Loss: 1.2204\n",
      "Epoch [8/10], Step [3100/18360], Loss: 1.2000\n",
      "Epoch [8/10], Step [3200/18360], Loss: 1.2039\n",
      "Epoch [8/10], Step [3300/18360], Loss: 1.1959\n",
      "Epoch [8/10], Step [3400/18360], Loss: 1.2118\n",
      "Epoch [8/10], Step [3500/18360], Loss: 1.1886\n",
      "Epoch [8/10], Step [3600/18360], Loss: 1.2084\n",
      "Epoch [8/10], Step [3700/18360], Loss: 1.2308\n",
      "Epoch [8/10], Step [3800/18360], Loss: 1.2076\n",
      "Epoch [8/10], Step [3900/18360], Loss: 1.2158\n",
      "Epoch [8/10], Step [4000/18360], Loss: 1.2138\n",
      "Epoch [8/10], Step [4100/18360], Loss: 1.1840\n",
      "Epoch [8/10], Step [4200/18360], Loss: 1.2235\n",
      "Epoch [8/10], Step [4300/18360], Loss: 1.2120\n",
      "Epoch [8/10], Step [4400/18360], Loss: 1.2333\n",
      "Epoch [8/10], Step [4500/18360], Loss: 1.1940\n",
      "Epoch [8/10], Step [4600/18360], Loss: 1.2135\n",
      "Epoch [8/10], Step [4700/18360], Loss: 1.2218\n",
      "Epoch [8/10], Step [4800/18360], Loss: 1.2271\n",
      "Epoch [8/10], Step [4900/18360], Loss: 1.2390\n",
      "Epoch [8/10], Step [5000/18360], Loss: 1.2290\n",
      "Epoch [8/10], Step [5100/18360], Loss: 1.2319\n",
      "Epoch [8/10], Step [5200/18360], Loss: 1.2299\n",
      "Epoch [8/10], Step [5300/18360], Loss: 1.2375\n",
      "Epoch [8/10], Step [5400/18360], Loss: 1.2197\n",
      "Epoch [8/10], Step [5500/18360], Loss: 1.2479\n",
      "Epoch [8/10], Step [5600/18360], Loss: 1.2313\n",
      "Epoch [8/10], Step [5700/18360], Loss: 1.2298\n",
      "Epoch [8/10], Step [5800/18360], Loss: 1.2423\n",
      "Epoch [8/10], Step [5900/18360], Loss: 1.2349\n",
      "Epoch [8/10], Step [6000/18360], Loss: 1.2321\n",
      "Epoch [8/10], Step [6100/18360], Loss: 1.2345\n",
      "Epoch [8/10], Step [6200/18360], Loss: 1.2477\n",
      "Epoch [8/10], Step [6300/18360], Loss: 1.2459\n",
      "Epoch [8/10], Step [6400/18360], Loss: 1.2566\n",
      "Epoch [8/10], Step [6500/18360], Loss: 1.2583\n",
      "Epoch [8/10], Step [6600/18360], Loss: 1.2380\n",
      "Epoch [8/10], Step [6700/18360], Loss: 1.2388\n",
      "Epoch [8/10], Step [6800/18360], Loss: 1.2286\n",
      "Epoch [8/10], Step [6900/18360], Loss: 1.2458\n",
      "Epoch [8/10], Step [7000/18360], Loss: 1.2560\n",
      "Epoch [8/10], Step [7100/18360], Loss: 1.2736\n",
      "Epoch [8/10], Step [7200/18360], Loss: 1.2682\n",
      "Epoch [8/10], Step [7300/18360], Loss: 1.2444\n",
      "Epoch [8/10], Step [7400/18360], Loss: 1.2472\n",
      "Epoch [8/10], Step [7500/18360], Loss: 1.2547\n",
      "Epoch [8/10], Step [7600/18360], Loss: 1.2566\n",
      "Epoch [8/10], Step [7700/18360], Loss: 1.2279\n",
      "Epoch [8/10], Step [7800/18360], Loss: 1.2773\n",
      "Epoch [8/10], Step [7900/18360], Loss: 1.2664\n",
      "Epoch [8/10], Step [8000/18360], Loss: 1.2630\n",
      "Epoch [8/10], Step [8100/18360], Loss: 1.2660\n",
      "Epoch [8/10], Step [8200/18360], Loss: 1.2608\n",
      "Epoch [8/10], Step [8300/18360], Loss: 1.2712\n",
      "Epoch [8/10], Step [8400/18360], Loss: 1.2654\n",
      "Epoch [8/10], Step [8500/18360], Loss: 1.2570\n",
      "Epoch [8/10], Step [8600/18360], Loss: 1.2907\n",
      "Epoch [8/10], Step [8700/18360], Loss: 1.2999\n",
      "Epoch [8/10], Step [8800/18360], Loss: 1.2580\n",
      "Epoch [8/10], Step [8900/18360], Loss: 1.2857\n",
      "Epoch [8/10], Step [9000/18360], Loss: 1.2689\n",
      "Epoch [8/10], Step [9100/18360], Loss: 1.2700\n",
      "Epoch [8/10], Step [9200/18360], Loss: 1.2843\n",
      "Epoch [8/10], Step [9300/18360], Loss: 1.2747\n",
      "Epoch [8/10], Step [9400/18360], Loss: 1.2735\n",
      "Epoch [8/10], Step [9500/18360], Loss: 1.2881\n",
      "Epoch [8/10], Step [9600/18360], Loss: 1.2701\n",
      "Epoch [8/10], Step [9700/18360], Loss: 1.2738\n",
      "Epoch [8/10], Step [9800/18360], Loss: 1.2818\n",
      "Epoch [8/10], Step [9900/18360], Loss: 1.2897\n",
      "Epoch [8/10], Step [10000/18360], Loss: 1.2758\n",
      "Epoch [8/10], Step [10100/18360], Loss: 1.2830\n",
      "Epoch [8/10], Step [10200/18360], Loss: 1.2773\n",
      "Epoch [8/10], Step [10300/18360], Loss: 1.2590\n",
      "Epoch [8/10], Step [10400/18360], Loss: 1.2699\n",
      "Epoch [8/10], Step [10500/18360], Loss: 1.2824\n",
      "Epoch [8/10], Step [10600/18360], Loss: 1.2784\n",
      "Epoch [8/10], Step [10700/18360], Loss: 1.2883\n",
      "Epoch [8/10], Step [10800/18360], Loss: 1.2801\n",
      "Epoch [8/10], Step [10900/18360], Loss: 1.3005\n",
      "Epoch [8/10], Step [11000/18360], Loss: 1.3051\n",
      "Epoch [8/10], Step [11100/18360], Loss: 1.2820\n",
      "Epoch [8/10], Step [11200/18360], Loss: 1.2920\n",
      "Epoch [8/10], Step [11300/18360], Loss: 1.3024\n",
      "Epoch [8/10], Step [11400/18360], Loss: 1.2746\n",
      "Epoch [8/10], Step [11500/18360], Loss: 1.3002\n",
      "Epoch [8/10], Step [11600/18360], Loss: 1.2899\n",
      "Epoch [8/10], Step [11700/18360], Loss: 1.3038\n",
      "Epoch [8/10], Step [11800/18360], Loss: 1.2937\n",
      "Epoch [8/10], Step [11900/18360], Loss: 1.2829\n",
      "Epoch [8/10], Step [12000/18360], Loss: 1.3173\n",
      "Epoch [8/10], Step [12100/18360], Loss: 1.3076\n",
      "Epoch [8/10], Step [12200/18360], Loss: 1.3069\n",
      "Epoch [8/10], Step [12300/18360], Loss: 1.2913\n",
      "Epoch [8/10], Step [12400/18360], Loss: 1.2989\n",
      "Epoch [8/10], Step [12500/18360], Loss: 1.3043\n",
      "Epoch [8/10], Step [12600/18360], Loss: 1.3023\n",
      "Epoch [8/10], Step [12700/18360], Loss: 1.3084\n",
      "Epoch [8/10], Step [12800/18360], Loss: 1.3051\n",
      "Epoch [8/10], Step [12900/18360], Loss: 1.2855\n",
      "Epoch [8/10], Step [13000/18360], Loss: 1.2878\n",
      "Epoch [8/10], Step [13100/18360], Loss: 1.3411\n",
      "Epoch [8/10], Step [13200/18360], Loss: 1.3047\n",
      "Epoch [8/10], Step [13300/18360], Loss: 1.3154\n",
      "Epoch [8/10], Step [13400/18360], Loss: 1.3263\n",
      "Epoch [8/10], Step [13500/18360], Loss: 1.3186\n",
      "Epoch [8/10], Step [13600/18360], Loss: 1.3230\n",
      "Epoch [8/10], Step [13700/18360], Loss: 1.3091\n",
      "Epoch [8/10], Step [13800/18360], Loss: 1.3164\n",
      "Epoch [8/10], Step [13900/18360], Loss: 1.3234\n",
      "Epoch [8/10], Step [14000/18360], Loss: 1.3376\n",
      "Epoch [8/10], Step [14100/18360], Loss: 1.3188\n",
      "Epoch [8/10], Step [14200/18360], Loss: 1.3058\n",
      "Epoch [8/10], Step [14300/18360], Loss: 1.3112\n",
      "Epoch [8/10], Step [14400/18360], Loss: 1.3126\n",
      "Epoch [8/10], Step [14500/18360], Loss: 1.3371\n",
      "Epoch [8/10], Step [14600/18360], Loss: 1.3139\n",
      "Epoch [8/10], Step [14700/18360], Loss: 1.3230\n",
      "Epoch [8/10], Step [14800/18360], Loss: 1.3217\n",
      "Epoch [8/10], Step [14900/18360], Loss: 1.3272\n",
      "Epoch [8/10], Step [15000/18360], Loss: 1.3254\n",
      "Epoch [8/10], Step [15100/18360], Loss: 1.3361\n",
      "Epoch [8/10], Step [15200/18360], Loss: 1.3255\n",
      "Epoch [8/10], Step [15300/18360], Loss: 1.3152\n",
      "Epoch [8/10], Step [15400/18360], Loss: 1.3359\n",
      "Epoch [8/10], Step [15500/18360], Loss: 1.3217\n",
      "Epoch [8/10], Step [15600/18360], Loss: 1.3413\n",
      "Epoch [8/10], Step [15700/18360], Loss: 1.3257\n",
      "Epoch [8/10], Step [15800/18360], Loss: 1.3204\n",
      "Epoch [8/10], Step [15900/18360], Loss: 1.3188\n",
      "Epoch [8/10], Step [16000/18360], Loss: 1.3391\n",
      "Epoch [8/10], Step [16100/18360], Loss: 1.3590\n",
      "Epoch [8/10], Step [16200/18360], Loss: 1.3386\n",
      "Epoch [8/10], Step [16300/18360], Loss: 1.3205\n",
      "Epoch [8/10], Step [16400/18360], Loss: 1.3259\n",
      "Epoch [8/10], Step [16500/18360], Loss: 1.3301\n",
      "Epoch [8/10], Step [16600/18360], Loss: 1.3358\n",
      "Epoch [8/10], Step [16700/18360], Loss: 1.3310\n",
      "Epoch [8/10], Step [16800/18360], Loss: 1.3477\n",
      "Epoch [8/10], Step [16900/18360], Loss: 1.3660\n",
      "Epoch [8/10], Step [17000/18360], Loss: 1.3351\n",
      "Epoch [8/10], Step [17100/18360], Loss: 1.3335\n",
      "Epoch [8/10], Step [17200/18360], Loss: 1.3469\n",
      "Epoch [8/10], Step [17300/18360], Loss: 1.3428\n",
      "Epoch [8/10], Step [17400/18360], Loss: 1.3192\n",
      "Epoch [8/10], Step [17500/18360], Loss: 1.3585\n",
      "Epoch [8/10], Step [17600/18360], Loss: 1.3534\n",
      "Epoch [8/10], Step [17700/18360], Loss: 1.3267\n",
      "Epoch [8/10], Step [17800/18360], Loss: 1.3315\n",
      "Epoch [8/10], Step [17900/18360], Loss: 1.3585\n",
      "Epoch [8/10], Step [18000/18360], Loss: 1.3529\n",
      "Epoch [8/10], Step [18100/18360], Loss: 1.3524\n",
      "Epoch [8/10], Step [18200/18360], Loss: 1.3578\n",
      "Epoch [8/10], Step [18300/18360], Loss: 1.3491\n",
      "Epoch [9/10], Step [100/18360], Loss: 1.0789\n",
      "Epoch [9/10], Step [200/18360], Loss: 1.0782\n",
      "Epoch [9/10], Step [300/18360], Loss: 1.0701\n",
      "Epoch [9/10], Step [400/18360], Loss: 1.0698\n",
      "Epoch [9/10], Step [500/18360], Loss: 1.0781\n",
      "Epoch [9/10], Step [600/18360], Loss: 1.0885\n",
      "Epoch [9/10], Step [700/18360], Loss: 1.0686\n",
      "Epoch [9/10], Step [800/18360], Loss: 1.0819\n",
      "Epoch [9/10], Step [900/18360], Loss: 1.0792\n",
      "Epoch [9/10], Step [1000/18360], Loss: 1.0896\n",
      "Epoch [9/10], Step [1100/18360], Loss: 1.0770\n",
      "Epoch [9/10], Step [1200/18360], Loss: 1.0940\n",
      "Epoch [9/10], Step [1300/18360], Loss: 1.0621\n",
      "Epoch [9/10], Step [1400/18360], Loss: 1.0800\n",
      "Epoch [9/10], Step [1500/18360], Loss: 1.0901\n",
      "Epoch [9/10], Step [1600/18360], Loss: 1.1284\n",
      "Epoch [9/10], Step [1700/18360], Loss: 1.0943\n",
      "Epoch [9/10], Step [1800/18360], Loss: 1.1132\n",
      "Epoch [9/10], Step [1900/18360], Loss: 1.0986\n",
      "Epoch [9/10], Step [2000/18360], Loss: 1.0752\n",
      "Epoch [9/10], Step [2100/18360], Loss: 1.0938\n",
      "Epoch [9/10], Step [2200/18360], Loss: 1.1043\n",
      "Epoch [9/10], Step [2300/18360], Loss: 1.0777\n",
      "Epoch [9/10], Step [2400/18360], Loss: 1.0791\n",
      "Epoch [9/10], Step [2500/18360], Loss: 1.1045\n",
      "Epoch [9/10], Step [2600/18360], Loss: 1.0924\n",
      "Epoch [9/10], Step [2700/18360], Loss: 1.1086\n",
      "Epoch [9/10], Step [2800/18360], Loss: 1.1224\n",
      "Epoch [9/10], Step [2900/18360], Loss: 1.1240\n",
      "Epoch [9/10], Step [3000/18360], Loss: 1.1084\n",
      "Epoch [9/10], Step [3100/18360], Loss: 1.0916\n",
      "Epoch [9/10], Step [3200/18360], Loss: 1.1210\n",
      "Epoch [9/10], Step [3300/18360], Loss: 1.1251\n",
      "Epoch [9/10], Step [3400/18360], Loss: 1.0891\n",
      "Epoch [9/10], Step [3500/18360], Loss: 1.1349\n",
      "Epoch [9/10], Step [3600/18360], Loss: 1.1199\n",
      "Epoch [9/10], Step [3700/18360], Loss: 1.1335\n",
      "Epoch [9/10], Step [3800/18360], Loss: 1.1238\n",
      "Epoch [9/10], Step [3900/18360], Loss: 1.1431\n",
      "Epoch [9/10], Step [4000/18360], Loss: 1.1284\n",
      "Epoch [9/10], Step [4100/18360], Loss: 1.1433\n",
      "Epoch [9/10], Step [4200/18360], Loss: 1.1191\n",
      "Epoch [9/10], Step [4300/18360], Loss: 1.1294\n",
      "Epoch [9/10], Step [4400/18360], Loss: 1.1406\n",
      "Epoch [9/10], Step [4500/18360], Loss: 1.1339\n",
      "Epoch [9/10], Step [4600/18360], Loss: 1.1494\n",
      "Epoch [9/10], Step [4700/18360], Loss: 1.1330\n",
      "Epoch [9/10], Step [4800/18360], Loss: 1.1400\n",
      "Epoch [9/10], Step [4900/18360], Loss: 1.1359\n",
      "Epoch [9/10], Step [5000/18360], Loss: 1.1435\n",
      "Epoch [9/10], Step [5100/18360], Loss: 1.1444\n",
      "Epoch [9/10], Step [5200/18360], Loss: 1.1351\n",
      "Epoch [9/10], Step [5300/18360], Loss: 1.1552\n",
      "Epoch [9/10], Step [5400/18360], Loss: 1.1384\n",
      "Epoch [9/10], Step [5500/18360], Loss: 1.1496\n",
      "Epoch [9/10], Step [5600/18360], Loss: 1.1512\n",
      "Epoch [9/10], Step [5700/18360], Loss: 1.1596\n",
      "Epoch [9/10], Step [5800/18360], Loss: 1.1516\n",
      "Epoch [9/10], Step [5900/18360], Loss: 1.1397\n",
      "Epoch [9/10], Step [6000/18360], Loss: 1.1584\n",
      "Epoch [9/10], Step [6100/18360], Loss: 1.1696\n",
      "Epoch [9/10], Step [6200/18360], Loss: 1.1688\n",
      "Epoch [9/10], Step [6300/18360], Loss: 1.1512\n",
      "Epoch [9/10], Step [6400/18360], Loss: 1.1919\n",
      "Epoch [9/10], Step [6500/18360], Loss: 1.1564\n",
      "Epoch [9/10], Step [6600/18360], Loss: 1.1796\n",
      "Epoch [9/10], Step [6700/18360], Loss: 1.1734\n",
      "Epoch [9/10], Step [6800/18360], Loss: 1.1586\n",
      "Epoch [9/10], Step [6900/18360], Loss: 1.1484\n",
      "Epoch [9/10], Step [7000/18360], Loss: 1.1585\n",
      "Epoch [9/10], Step [7100/18360], Loss: 1.1810\n",
      "Epoch [9/10], Step [7200/18360], Loss: 1.1690\n",
      "Epoch [9/10], Step [7300/18360], Loss: 1.1627\n",
      "Epoch [9/10], Step [7400/18360], Loss: 1.1988\n",
      "Epoch [9/10], Step [7500/18360], Loss: 1.1834\n",
      "Epoch [9/10], Step [7600/18360], Loss: 1.1806\n",
      "Epoch [9/10], Step [7700/18360], Loss: 1.1598\n",
      "Epoch [9/10], Step [7800/18360], Loss: 1.1820\n",
      "Epoch [9/10], Step [7900/18360], Loss: 1.1864\n",
      "Epoch [9/10], Step [8000/18360], Loss: 1.1841\n",
      "Epoch [9/10], Step [8100/18360], Loss: 1.1628\n",
      "Epoch [9/10], Step [8200/18360], Loss: 1.1763\n",
      "Epoch [9/10], Step [8300/18360], Loss: 1.1839\n",
      "Epoch [9/10], Step [8400/18360], Loss: 1.1767\n",
      "Epoch [9/10], Step [8500/18360], Loss: 1.2149\n",
      "Epoch [9/10], Step [8600/18360], Loss: 1.1804\n",
      "Epoch [9/10], Step [8700/18360], Loss: 1.1756\n",
      "Epoch [9/10], Step [8800/18360], Loss: 1.1921\n",
      "Epoch [9/10], Step [8900/18360], Loss: 1.2018\n",
      "Epoch [9/10], Step [9000/18360], Loss: 1.2123\n",
      "Epoch [9/10], Step [9100/18360], Loss: 1.2052\n",
      "Epoch [9/10], Step [9200/18360], Loss: 1.1939\n",
      "Epoch [9/10], Step [9300/18360], Loss: 1.1915\n",
      "Epoch [9/10], Step [9400/18360], Loss: 1.1911\n",
      "Epoch [9/10], Step [9500/18360], Loss: 1.2009\n",
      "Epoch [9/10], Step [9600/18360], Loss: 1.1937\n",
      "Epoch [9/10], Step [9700/18360], Loss: 1.2123\n",
      "Epoch [9/10], Step [9800/18360], Loss: 1.1902\n",
      "Epoch [9/10], Step [9900/18360], Loss: 1.1918\n",
      "Epoch [9/10], Step [10000/18360], Loss: 1.1847\n",
      "Epoch [9/10], Step [10100/18360], Loss: 1.2147\n",
      "Epoch [9/10], Step [10200/18360], Loss: 1.2093\n",
      "Epoch [9/10], Step [10300/18360], Loss: 1.2022\n",
      "Epoch [9/10], Step [10400/18360], Loss: 1.1975\n",
      "Epoch [9/10], Step [10500/18360], Loss: 1.2272\n",
      "Epoch [9/10], Step [10600/18360], Loss: 1.2076\n",
      "Epoch [9/10], Step [10700/18360], Loss: 1.2129\n",
      "Epoch [9/10], Step [10800/18360], Loss: 1.2103\n",
      "Epoch [9/10], Step [10900/18360], Loss: 1.2135\n",
      "Epoch [9/10], Step [11000/18360], Loss: 1.2301\n",
      "Epoch [9/10], Step [11100/18360], Loss: 1.2144\n",
      "Epoch [9/10], Step [11200/18360], Loss: 1.2269\n",
      "Epoch [9/10], Step [11300/18360], Loss: 1.1900\n",
      "Epoch [9/10], Step [11400/18360], Loss: 1.2159\n",
      "Epoch [9/10], Step [11500/18360], Loss: 1.2073\n",
      "Epoch [9/10], Step [11600/18360], Loss: 1.2225\n",
      "Epoch [9/10], Step [11700/18360], Loss: 1.2191\n",
      "Epoch [9/10], Step [11800/18360], Loss: 1.2047\n",
      "Epoch [9/10], Step [11900/18360], Loss: 1.2389\n",
      "Epoch [9/10], Step [12000/18360], Loss: 1.2041\n",
      "Epoch [9/10], Step [12100/18360], Loss: 1.2227\n",
      "Epoch [9/10], Step [12200/18360], Loss: 1.2168\n",
      "Epoch [9/10], Step [12300/18360], Loss: 1.2121\n",
      "Epoch [9/10], Step [12400/18360], Loss: 1.2126\n",
      "Epoch [9/10], Step [12500/18360], Loss: 1.2123\n",
      "Epoch [9/10], Step [12600/18360], Loss: 1.2234\n",
      "Epoch [9/10], Step [12700/18360], Loss: 1.2327\n",
      "Epoch [9/10], Step [12800/18360], Loss: 1.2254\n",
      "Epoch [9/10], Step [12900/18360], Loss: 1.2377\n",
      "Epoch [9/10], Step [13000/18360], Loss: 1.2245\n",
      "Epoch [9/10], Step [13100/18360], Loss: 1.2475\n",
      "Epoch [9/10], Step [13200/18360], Loss: 1.2226\n",
      "Epoch [9/10], Step [13300/18360], Loss: 1.2365\n",
      "Epoch [9/10], Step [13400/18360], Loss: 1.2327\n",
      "Epoch [9/10], Step [13500/18360], Loss: 1.2287\n",
      "Epoch [9/10], Step [13600/18360], Loss: 1.2242\n",
      "Epoch [9/10], Step [13700/18360], Loss: 1.2415\n",
      "Epoch [9/10], Step [13800/18360], Loss: 1.2427\n",
      "Epoch [9/10], Step [13900/18360], Loss: 1.2392\n",
      "Epoch [9/10], Step [14000/18360], Loss: 1.2455\n",
      "Epoch [9/10], Step [14100/18360], Loss: 1.2251\n",
      "Epoch [9/10], Step [14200/18360], Loss: 1.2432\n",
      "Epoch [9/10], Step [14300/18360], Loss: 1.2366\n",
      "Epoch [9/10], Step [14400/18360], Loss: 1.2333\n",
      "Epoch [9/10], Step [14500/18360], Loss: 1.2323\n",
      "Epoch [9/10], Step [14600/18360], Loss: 1.2466\n",
      "Epoch [9/10], Step [14700/18360], Loss: 1.2434\n",
      "Epoch [9/10], Step [14800/18360], Loss: 1.2297\n",
      "Epoch [9/10], Step [14900/18360], Loss: 1.2351\n",
      "Epoch [9/10], Step [15000/18360], Loss: 1.2363\n",
      "Epoch [9/10], Step [15100/18360], Loss: 1.2338\n",
      "Epoch [9/10], Step [15200/18360], Loss: 1.2302\n",
      "Epoch [9/10], Step [15300/18360], Loss: 1.2256\n",
      "Epoch [9/10], Step [15400/18360], Loss: 1.2520\n",
      "Epoch [9/10], Step [15500/18360], Loss: 1.2232\n",
      "Epoch [9/10], Step [15600/18360], Loss: 1.2519\n",
      "Epoch [9/10], Step [15700/18360], Loss: 1.2570\n",
      "Epoch [9/10], Step [15800/18360], Loss: 1.2501\n",
      "Epoch [9/10], Step [15900/18360], Loss: 1.2335\n",
      "Epoch [9/10], Step [16000/18360], Loss: 1.2627\n",
      "Epoch [9/10], Step [16100/18360], Loss: 1.2511\n",
      "Epoch [9/10], Step [16200/18360], Loss: 1.2596\n",
      "Epoch [9/10], Step [16300/18360], Loss: 1.2552\n",
      "Epoch [9/10], Step [16400/18360], Loss: 1.2366\n",
      "Epoch [9/10], Step [16500/18360], Loss: 1.2577\n",
      "Epoch [9/10], Step [16600/18360], Loss: 1.2529\n",
      "Epoch [9/10], Step [16700/18360], Loss: 1.2537\n",
      "Epoch [9/10], Step [16800/18360], Loss: 1.2527\n",
      "Epoch [9/10], Step [16900/18360], Loss: 1.2516\n",
      "Epoch [9/10], Step [17000/18360], Loss: 1.2314\n",
      "Epoch [9/10], Step [17100/18360], Loss: 1.2562\n",
      "Epoch [9/10], Step [17200/18360], Loss: 1.2429\n",
      "Epoch [9/10], Step [17300/18360], Loss: 1.2647\n",
      "Epoch [9/10], Step [17400/18360], Loss: 1.2605\n",
      "Epoch [9/10], Step [17500/18360], Loss: 1.2659\n",
      "Epoch [9/10], Step [17600/18360], Loss: 1.2616\n",
      "Epoch [9/10], Step [17700/18360], Loss: 1.2514\n",
      "Epoch [9/10], Step [17800/18360], Loss: 1.2629\n",
      "Epoch [9/10], Step [17900/18360], Loss: 1.2569\n",
      "Epoch [9/10], Step [18000/18360], Loss: 1.2705\n",
      "Epoch [9/10], Step [18100/18360], Loss: 1.2776\n",
      "Epoch [9/10], Step [18200/18360], Loss: 1.2452\n",
      "Epoch [9/10], Step [18300/18360], Loss: 1.2522\n",
      "Epoch [10/10], Step [100/18360], Loss: 1.0125\n",
      "Epoch [10/10], Step [200/18360], Loss: 0.9878\n",
      "Epoch [10/10], Step [300/18360], Loss: 0.9827\n",
      "Epoch [10/10], Step [400/18360], Loss: 0.9915\n",
      "Epoch [10/10], Step [500/18360], Loss: 0.9827\n",
      "Epoch [10/10], Step [600/18360], Loss: 1.0014\n",
      "Epoch [10/10], Step [700/18360], Loss: 0.9863\n",
      "Epoch [10/10], Step [800/18360], Loss: 0.9926\n",
      "Epoch [10/10], Step [900/18360], Loss: 1.0024\n",
      "Epoch [10/10], Step [1000/18360], Loss: 1.0137\n",
      "Epoch [10/10], Step [1100/18360], Loss: 1.0093\n",
      "Epoch [10/10], Step [1200/18360], Loss: 0.9894\n",
      "Epoch [10/10], Step [1300/18360], Loss: 1.0080\n",
      "Epoch [10/10], Step [1400/18360], Loss: 1.0054\n",
      "Epoch [10/10], Step [1500/18360], Loss: 1.0007\n",
      "Epoch [10/10], Step [1600/18360], Loss: 0.9998\n",
      "Epoch [10/10], Step [1700/18360], Loss: 1.0250\n",
      "Epoch [10/10], Step [1800/18360], Loss: 1.0099\n",
      "Epoch [10/10], Step [1900/18360], Loss: 1.0443\n",
      "Epoch [10/10], Step [2000/18360], Loss: 1.0129\n",
      "Epoch [10/10], Step [2100/18360], Loss: 1.0031\n",
      "Epoch [10/10], Step [2200/18360], Loss: 1.0392\n",
      "Epoch [10/10], Step [2300/18360], Loss: 1.0291\n",
      "Epoch [10/10], Step [2400/18360], Loss: 1.0252\n",
      "Epoch [10/10], Step [2500/18360], Loss: 1.0198\n",
      "Epoch [10/10], Step [2600/18360], Loss: 1.0182\n",
      "Epoch [10/10], Step [2700/18360], Loss: 1.0155\n",
      "Epoch [10/10], Step [2800/18360], Loss: 1.0469\n",
      "Epoch [10/10], Step [2900/18360], Loss: 1.0465\n",
      "Epoch [10/10], Step [3000/18360], Loss: 1.0403\n",
      "Epoch [10/10], Step [3100/18360], Loss: 1.0293\n",
      "Epoch [10/10], Step [3200/18360], Loss: 1.0481\n",
      "Epoch [10/10], Step [3300/18360], Loss: 1.0377\n",
      "Epoch [10/10], Step [3400/18360], Loss: 1.0352\n",
      "Epoch [10/10], Step [3500/18360], Loss: 1.0444\n",
      "Epoch [10/10], Step [3600/18360], Loss: 1.0518\n",
      "Epoch [10/10], Step [3700/18360], Loss: 1.0486\n",
      "Epoch [10/10], Step [3800/18360], Loss: 1.0597\n",
      "Epoch [10/10], Step [3900/18360], Loss: 1.0637\n",
      "Epoch [10/10], Step [4000/18360], Loss: 1.0699\n",
      "Epoch [10/10], Step [4100/18360], Loss: 1.0650\n",
      "Epoch [10/10], Step [4200/18360], Loss: 1.0585\n",
      "Epoch [10/10], Step [4300/18360], Loss: 1.0453\n",
      "Epoch [10/10], Step [4400/18360], Loss: 1.0566\n",
      "Epoch [10/10], Step [4500/18360], Loss: 1.0414\n",
      "Epoch [10/10], Step [4600/18360], Loss: 1.0566\n",
      "Epoch [10/10], Step [4700/18360], Loss: 1.0798\n",
      "Epoch [10/10], Step [4800/18360], Loss: 1.0690\n",
      "Epoch [10/10], Step [4900/18360], Loss: 1.0741\n",
      "Epoch [10/10], Step [5000/18360], Loss: 1.0569\n",
      "Epoch [10/10], Step [5100/18360], Loss: 1.0750\n",
      "Epoch [10/10], Step [5200/18360], Loss: 1.0688\n",
      "Epoch [10/10], Step [5300/18360], Loss: 1.0657\n",
      "Epoch [10/10], Step [5400/18360], Loss: 1.0719\n",
      "Epoch [10/10], Step [5500/18360], Loss: 1.0748\n",
      "Epoch [10/10], Step [5600/18360], Loss: 1.0420\n",
      "Epoch [10/10], Step [5700/18360], Loss: 1.0769\n",
      "Epoch [10/10], Step [5800/18360], Loss: 1.0629\n",
      "Epoch [10/10], Step [5900/18360], Loss: 1.0759\n",
      "Epoch [10/10], Step [6000/18360], Loss: 1.0709\n",
      "Epoch [10/10], Step [6100/18360], Loss: 1.0870\n",
      "Epoch [10/10], Step [6200/18360], Loss: 1.0839\n",
      "Epoch [10/10], Step [6300/18360], Loss: 1.0878\n",
      "Epoch [10/10], Step [6400/18360], Loss: 1.0618\n",
      "Epoch [10/10], Step [6500/18360], Loss: 1.0922\n",
      "Epoch [10/10], Step [6600/18360], Loss: 1.0825\n",
      "Epoch [10/10], Step [6700/18360], Loss: 1.0881\n",
      "Epoch [10/10], Step [6800/18360], Loss: 1.0894\n",
      "Epoch [10/10], Step [6900/18360], Loss: 1.0950\n",
      "Epoch [10/10], Step [7000/18360], Loss: 1.0838\n",
      "Epoch [10/10], Step [7100/18360], Loss: 1.1039\n",
      "Epoch [10/10], Step [7200/18360], Loss: 1.0905\n",
      "Epoch [10/10], Step [7300/18360], Loss: 1.0869\n",
      "Epoch [10/10], Step [7400/18360], Loss: 1.1187\n",
      "Epoch [10/10], Step [7500/18360], Loss: 1.0889\n",
      "Epoch [10/10], Step [7600/18360], Loss: 1.1045\n",
      "Epoch [10/10], Step [7700/18360], Loss: 1.0836\n",
      "Epoch [10/10], Step [7800/18360], Loss: 1.0698\n",
      "Epoch [10/10], Step [7900/18360], Loss: 1.0948\n",
      "Epoch [10/10], Step [8000/18360], Loss: 1.1040\n",
      "Epoch [10/10], Step [8100/18360], Loss: 1.1108\n",
      "Epoch [10/10], Step [8200/18360], Loss: 1.1209\n",
      "Epoch [10/10], Step [8300/18360], Loss: 1.0949\n",
      "Epoch [10/10], Step [8400/18360], Loss: 1.1074\n",
      "Epoch [10/10], Step [8500/18360], Loss: 1.1203\n",
      "Epoch [10/10], Step [8600/18360], Loss: 1.1031\n",
      "Epoch [10/10], Step [8700/18360], Loss: 1.1122\n",
      "Epoch [10/10], Step [8800/18360], Loss: 1.1231\n",
      "Epoch [10/10], Step [8900/18360], Loss: 1.1074\n",
      "Epoch [10/10], Step [9000/18360], Loss: 1.1104\n",
      "Epoch [10/10], Step [9100/18360], Loss: 1.1141\n",
      "Epoch [10/10], Step [9200/18360], Loss: 1.1225\n",
      "Epoch [10/10], Step [9300/18360], Loss: 1.1337\n",
      "Epoch [10/10], Step [9400/18360], Loss: 1.1241\n",
      "Epoch [10/10], Step [9500/18360], Loss: 1.1242\n",
      "Epoch [10/10], Step [9600/18360], Loss: 1.1183\n",
      "Epoch [10/10], Step [9700/18360], Loss: 1.1173\n",
      "Epoch [10/10], Step [9800/18360], Loss: 1.1260\n",
      "Epoch [10/10], Step [9900/18360], Loss: 1.1279\n",
      "Epoch [10/10], Step [10000/18360], Loss: 1.1306\n",
      "Epoch [10/10], Step [10100/18360], Loss: 1.1216\n",
      "Epoch [10/10], Step [10200/18360], Loss: 1.1312\n",
      "Epoch [10/10], Step [10300/18360], Loss: 1.1296\n",
      "Epoch [10/10], Step [10400/18360], Loss: 1.1251\n",
      "Epoch [10/10], Step [10500/18360], Loss: 1.1356\n",
      "Epoch [10/10], Step [10600/18360], Loss: 1.1431\n",
      "Epoch [10/10], Step [10700/18360], Loss: 1.1526\n",
      "Epoch [10/10], Step [10800/18360], Loss: 1.1256\n",
      "Epoch [10/10], Step [10900/18360], Loss: 1.1430\n",
      "Epoch [10/10], Step [11000/18360], Loss: 1.1318\n",
      "Epoch [10/10], Step [11100/18360], Loss: 1.1444\n",
      "Epoch [10/10], Step [11200/18360], Loss: 1.1158\n",
      "Epoch [10/10], Step [11300/18360], Loss: 1.1399\n",
      "Epoch [10/10], Step [11400/18360], Loss: 1.1387\n",
      "Epoch [10/10], Step [11500/18360], Loss: 1.1143\n",
      "Epoch [10/10], Step [11600/18360], Loss: 1.1595\n",
      "Epoch [10/10], Step [11700/18360], Loss: 1.1454\n",
      "Epoch [10/10], Step [11800/18360], Loss: 1.1249\n",
      "Epoch [10/10], Step [11900/18360], Loss: 1.1465\n",
      "Epoch [10/10], Step [12000/18360], Loss: 1.1393\n",
      "Epoch [10/10], Step [12100/18360], Loss: 1.1309\n",
      "Epoch [10/10], Step [12200/18360], Loss: 1.1538\n",
      "Epoch [10/10], Step [12300/18360], Loss: 1.1486\n",
      "Epoch [10/10], Step [12400/18360], Loss: 1.1464\n",
      "Epoch [10/10], Step [12500/18360], Loss: 1.1455\n",
      "Epoch [10/10], Step [12600/18360], Loss: 1.1443\n",
      "Epoch [10/10], Step [12700/18360], Loss: 1.1507\n",
      "Epoch [10/10], Step [12800/18360], Loss: 1.1526\n",
      "Epoch [10/10], Step [12900/18360], Loss: 1.1634\n",
      "Epoch [10/10], Step [13000/18360], Loss: 1.1460\n",
      "Epoch [10/10], Step [13100/18360], Loss: 1.1555\n",
      "Epoch [10/10], Step [13200/18360], Loss: 1.1464\n",
      "Epoch [10/10], Step [13300/18360], Loss: 1.1480\n",
      "Epoch [10/10], Step [13400/18360], Loss: 1.1490\n",
      "Epoch [10/10], Step [13500/18360], Loss: 1.1473\n",
      "Epoch [10/10], Step [13600/18360], Loss: 1.1398\n",
      "Epoch [10/10], Step [13700/18360], Loss: 1.1566\n",
      "Epoch [10/10], Step [13800/18360], Loss: 1.1589\n",
      "Epoch [10/10], Step [13900/18360], Loss: 1.1514\n",
      "Epoch [10/10], Step [14000/18360], Loss: 1.1546\n",
      "Epoch [10/10], Step [14100/18360], Loss: 1.1362\n",
      "Epoch [10/10], Step [14200/18360], Loss: 1.1509\n",
      "Epoch [10/10], Step [14300/18360], Loss: 1.1627\n",
      "Epoch [10/10], Step [14400/18360], Loss: 1.1793\n",
      "Epoch [10/10], Step [14500/18360], Loss: 1.1745\n",
      "Epoch [10/10], Step [14600/18360], Loss: 1.1753\n",
      "Epoch [10/10], Step [14700/18360], Loss: 1.1483\n",
      "Epoch [10/10], Step [14800/18360], Loss: 1.1407\n",
      "Epoch [10/10], Step [14900/18360], Loss: 1.1858\n",
      "Epoch [10/10], Step [15000/18360], Loss: 1.1566\n",
      "Epoch [10/10], Step [15100/18360], Loss: 1.1642\n",
      "Epoch [10/10], Step [15200/18360], Loss: 1.1594\n",
      "Epoch [10/10], Step [15300/18360], Loss: 1.1734\n",
      "Epoch [10/10], Step [15400/18360], Loss: 1.1707\n",
      "Epoch [10/10], Step [15500/18360], Loss: 1.1819\n",
      "Epoch [10/10], Step [15600/18360], Loss: 1.1628\n",
      "Epoch [10/10], Step [15700/18360], Loss: 1.1699\n",
      "Epoch [10/10], Step [15800/18360], Loss: 1.1713\n",
      "Epoch [10/10], Step [15900/18360], Loss: 1.1632\n",
      "Epoch [10/10], Step [16000/18360], Loss: 1.1794\n",
      "Epoch [10/10], Step [16100/18360], Loss: 1.2039\n",
      "Epoch [10/10], Step [16200/18360], Loss: 1.1747\n",
      "Epoch [10/10], Step [16300/18360], Loss: 1.1696\n",
      "Epoch [10/10], Step [16400/18360], Loss: 1.1696\n",
      "Epoch [10/10], Step [16500/18360], Loss: 1.1720\n",
      "Epoch [10/10], Step [16600/18360], Loss: 1.1757\n",
      "Epoch [10/10], Step [16700/18360], Loss: 1.1955\n",
      "Epoch [10/10], Step [16800/18360], Loss: 1.1742\n",
      "Epoch [10/10], Step [16900/18360], Loss: 1.1732\n",
      "Epoch [10/10], Step [17000/18360], Loss: 1.1946\n",
      "Epoch [10/10], Step [17100/18360], Loss: 1.1664\n",
      "Epoch [10/10], Step [17200/18360], Loss: 1.1790\n",
      "Epoch [10/10], Step [17300/18360], Loss: 1.1781\n",
      "Epoch [10/10], Step [17400/18360], Loss: 1.1860\n",
      "Epoch [10/10], Step [17500/18360], Loss: 1.1939\n",
      "Epoch [10/10], Step [17600/18360], Loss: 1.1897\n",
      "Epoch [10/10], Step [17700/18360], Loss: 1.1931\n",
      "Epoch [10/10], Step [17800/18360], Loss: 1.1702\n",
      "Epoch [10/10], Step [17900/18360], Loss: 1.1729\n",
      "Epoch [10/10], Step [18000/18360], Loss: 1.1901\n",
      "Epoch [10/10], Step [18100/18360], Loss: 1.2072\n",
      "Epoch [10/10], Step [18200/18360], Loss: 1.1890\n",
      "Epoch [10/10], Step [18300/18360], Loss: 1.2194\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (batch_x, batch_y) in enumerate(loader):\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{batch_idx+1}/{len(loader)}], Loss: {running_loss/100:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scaler_state\": scaler.state_dict(),\n",
    "        \"epoch\": epoch\n",
    "    }, f\"chess_policy_epoch{epoch+1}_v4.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb85611",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Named (.pt)</th>\n",
    "    <th style=text-align:center>Model</th>\n",
    "    <th>Epochs</th>\n",
    "    <th>Precision</th>\n",
    "    <th>Learning rate</th>\n",
    "    <th>Positions</th>\n",
    "    <th>Loss</th>\n",
    "    <th>Best accuracy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=center>AlphaChess</td>\n",
    "    <td style=text-align:left>ChessPolicyNet</td>\n",
    "    <td align=center>5</td>\n",
    "    <td align=center>float32</td>\n",
    "    <td align=center>0,0003</td>\n",
    "    <td align=center>780 032</td>\n",
    "    <td align=center>1.5345</td>\n",
    "    <td align=center>79.17%</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=center>BetaChess</td>\n",
    "    <td style=text-align:left>ChessPolicyNet</td>\n",
    "    <td align=center>6</td>\n",
    "    <td align=center>float32</td>\n",
    "    <td align=center>0,0003</td>\n",
    "    <td align=center>1 560 064</td>\n",
    "    <td align=center>1.4802</td>\n",
    "    <td align=center>75.00%</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=center>GammaChess</td>\n",
    "    <td style=text-align:left>ChessPolicyNet</td>\n",
    "    <td align=center>12</td>\n",
    "    <td align=center>float16</td>\n",
    "    <td align=center>0,0003</td>\n",
    "    <td align=center>2 350 080</td>\n",
    "    <td align=center>1.0552</td>\n",
    "    <td align=center>66.67%</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=center>DeltaChess</td>\n",
    "    <td style=text-align:left>ChessPolicyNet</td>\n",
    "    <td align=center>10</td>\n",
    "    <td align=center>float32</td>\n",
    "    <td align=center>0,0003</td>\n",
    "    <td align=center>2 350 080</td>\n",
    "    <td align=center>1.2194</td>\n",
    "    <td align=center>75.00%</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
